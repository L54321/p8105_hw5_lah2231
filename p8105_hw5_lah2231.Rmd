---
title: "p8105_hw5_lah2231"
output: github_document
date: "2024-11-12"
---
```{r}
library(tidyverse)
library(broom)
library(ggplot2)
library(dplyr)
library(purrr)
```


# Problem 2

## T-Test simulation function
```{r}
n <- 30
sigma <- 5
mu_values <- c(0, 1, 2, 3, 4, 5, 6)
num_simulations <- 5000
alpha <- 0.05

simulate_t_test <- function(mu) {
  sample_data <- rnorm(n, mean = mu, sd = sigma)
  
  test_result <- t.test(sample_data, mu = 0)
  
  result <- tidy(test_result) |>
    select(estimate, p.value)
  
  result
}
```
I made the function simulate_t_test to generate 30 samples froma a normal distribution with mean mu and standard deviation sigma and used broom::tidy to extract the estimate and p-value.


## Running Simulation
```{r}
simulation_results <- expand_grid(mu = mu_values, sim = 1:num_simulations) |>
  mutate(result = map(mu, simulate_t_test)) |>
  unnest(result)
```
I created a grid of mu values and simulation iterations and ran the function for each combination and used unnest. 

## Calculating Power
```{r}
power_results <- simulation_results |>
  group_by(mu) |>
  summarize(power = mean(p.value < alpha))
```
I calculated the power for each value of mu

## Plotting Power
```{r}
power_results |>
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  labs(x = "True Mean (mu)", y = "Power", title = "Power vs effect size (True mean")
```
I plotted the relationship betwen effect size (mu) and power

## Calculating averga Eestimate of mu_hat
```{r}
average_mu_hat <- simulation_results |>
  group_by(mu) |>
  summarize(
    avg_mu_hat = mean(estimate),
    avg_mu_hat_rejected = mean(estimate[p.value < alpha])
  )
```
I calculated the overall average of mu for each true mean and the average of mu for datasets where null hypothesis was rejected


## Plotting average estimate of mu_hat vs. true mean
```{r}
average_mu_hat |>
  ggplot(aes(x = mu)) +
  geom_line(aes(y = avg_mu_hat, color = "Average mu_hat")) +
  geom_line(aes(y = avg_mu_hat_rejected, color = "Average mu_hat (Rejected)")) +
  labs(x = "True mean (mu)", y = "Average estimate of mu_hat",
       title = "Average estimate of mu_hat vs. true mean") +
  scale_color_manual(values = c("Average mu_hat" = "orange", "Average mu_hat (rejected)" = "green"))
```
I plotted overall avergae estimate of mu against true mean and overlayed the average estimate of mu for rejected null hypothesis


# Problem 3

## Loading Data Homicides
```{r}
library(readr)

url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicide_data <- read_csv(url)

head(homicide_data)
```

```{r}
dim(homicide_data)
summary(homicide_data)
```

This dataset about homicides in major US cities contains `r ncol(homicide_data)` columns and `r nrow(homicide_data)` rows. There are `r sum(is.na(homicide_data))` missing values across the dataset. The columns include: `r names(homicide_data)`.

## Creating City State Column and finding total and unsolved homicides
```{r}
homicide_data <- mutate(homicide_data, city_state = paste(city, state, sep = ", "))

homicide_summary <- homicide_data |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

head(homicide_summary)
```

This table summarizes all homicides across the `r nrow(homicide_summary)` cities, including the total number of homicides (`r sum(pull(homicide_summary, total_homicides))`) with `r sum(pull(homicide_summary, unsolved_homicides))` unsolved cases.

## Estimating unsolved cases in Baltimore with confidence interval
```{r}
baltimore_data <- homicide_summary |>
  filter(city_state == "Baltimore, MD")

baltimore_prop_test <- prop.test(
  baltimore_data$unsolved_homicides,
  baltimore_data$total_homicides
)
baltimore_prop_test 

library(broom)

baltimore_summary <- broom::tidy(baltimore_prop_test) %>%
  select(estimate, conf.low, conf.high)
baltimore_summary
```


In Baltimore `r round(pull(baltimore_summary, estimate) * 100, 1)`% of homicides are unsolved with a confidence interval of `r round(pull(baltimore_summary, conf.low) * 100, 1)`% to `r round(pull(baltimore_summary, conf.high) * 100, 1)`%.

## Proportion tests for each city
```{r}
homicide_summary <- homicide_summary |>
  mutate(
    prop_test = map2(
      unsolved_homicides, 
      total_homicides, 
      ~prop.test(.x, .y)
    )
  )
```
## Confidence intervals
```{r}
homicide_summary <- homicide_summary |>
  mutate(
    prop_test_tidy = map(prop_test, tidy),
    estimate = map_dbl(prop_test_tidy, ~.x$estimate * 100),
    conf_low = map_dbl(prop_test_tidy, ~.x$conf.low * 100),
    conf_high = map_dbl(prop_test_tidy, ~.x$conf.high * 100)
  )
```

## Selecting relevant columns
```{r}
homicide_summary <- homicide_summary |>
  select(city_state, total_homicides, unsolved_homicides, estimate, conf_low, conf_high)
```

## Plot Homicide Proportion unsolved
```{r}
ggplot(homicide_summary, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high), width = 0.2) +
  coord_flip() +
  labs(
    x = "City",
    y = "Proportion of unsolved homicides (%)",
    title = "Proportion of unsolved homicides by city"
  )
```


We can see that the proportion of unsolved homicides across `r nrow(homicide_summary)` cities goes from `r homicide_summary |> pull(conf_low) |> min() |> round(1)`% to `r homicide_summary |> pull(conf_high) |> max() |> round(1)`%. This very large confidence interval is most probably due to differences in efficiency regarding case solving between the cities or due to low homicide counts in case numbers. Tulsa seems to have had only one (solved) homicide case in our dataset which contributes to the large confidence interval.

```{r}
outliers <- homicide_summary |>
  filter(estimate < 10 | estimate > 90)

outliers
```

```{r}
homicide_summary |>
  mutate(ci_width = conf_high - conf_low) |>
  ggplot(aes(x = total_homicides, y = ci_width)) +
  geom_point() +
  labs(x = "Homicides", y = "CI",
       title = "CI vs homicides")
```










